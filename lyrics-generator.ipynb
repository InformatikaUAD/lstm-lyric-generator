{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "517fb453",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout, Embedding\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29fb8504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed untuk reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Download dan Load Dataset\n",
    "# Kita akan menggunakan dataset dari beberapa sumber\n",
    "# Untuk tutorial ini, kita akan membuat fungsi untuk download dataset\n",
    "\n",
    "def download_poetry_dataset():\n",
    "    \"\"\"\n",
    "    Fungsi untuk download dataset puisi dari berbagai sumber\n",
    "    Dataset yang direkomendasikan:\n",
    "    1. Kaggle: Poems Dataset (NLP) - https://www.kaggle.com/datasets/michaelarman/poemsdataset\n",
    "    2. Kaggle: Gutenberg Poetry Dataset - https://www.kaggle.com/datasets/terminate9298/gutenberg-poetry-dataset\n",
    "    3. Kaggle: Poetry Foundation Poems - https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems\n",
    "    \"\"\"\n",
    "    print(\"Silakan download salah satu dataset berikut:\")\n",
    "    print(\"1. Kaggle: Poems Dataset (NLP) - https://www.kaggle.com/datasets/michaelarman/poemsdataset\")\n",
    "    print(\"2. Kaggle: Gutenberg Poetry Dataset - https://www.kaggle.com/datasets/terminate9298/gutenberg-poetry-dataset\")\n",
    "    print(\"3. Kaggle: Poetry Foundation Poems - https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems\")\n",
    "    print(\"4. Kaggle: 100-poems dataset - https://www.kaggle.com/datasets/imbikramsaha/poems\")\n",
    "    \n",
    "    # Untuk demo, kita akan menggunakan sample data\n",
    "    sample_poems = [\n",
    "        \"Roses are red, violets are blue, sugar is sweet, and so are you\",\n",
    "        \"The sun sets in the west, painting the sky with golden hues\",\n",
    "        \"In the quiet of the night, stars whisper ancient secrets\",\n",
    "        \"Mountains stand tall and proud, reaching for the endless sky\",\n",
    "        \"Ocean waves dance and play, singing songs of distant lands\",\n",
    "        \"Time flows like a river, carrying memories downstream\",\n",
    "        \"Love is a gentle breeze, touching hearts with tender care\",\n",
    "        \"Dreams take flight on wings of hope, soaring beyond reality\",\n",
    "        \"Flowers bloom in springtime, bringing joy to weary souls\",\n",
    "        \"Moonlight guides the lonely traveler through the darkest path\"\n",
    "    ]\n",
    "    \n",
    "    return sample_poems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38231ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocessing Data\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "        self.max_sequence_length = 0\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Membersihkan teks dari karakter yang tidak diinginkan\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove numbers (optional)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Keep only letters, spaces, and basic punctuation\n",
    "        text = re.sub(r'[^a-zA-Z\\s.,!?;:\\'\\-]', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def prepare_sequences(self, texts, vocab_size=5000, max_length=50):\n",
    "        \"\"\"Mempersiapkan sekuens untuk training\"\"\"\n",
    "        # Clean texts\n",
    "        cleaned_texts = [self.clean_text(text) for text in texts]\n",
    "        \n",
    "        # Tokenize\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "        self.tokenizer.fit_on_texts(cleaned_texts)\n",
    "        \n",
    "        # Convert to sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)\n",
    "        \n",
    "        # Create training sequences\n",
    "        input_sequences = []\n",
    "        for sequence in sequences:\n",
    "            for i in range(1, len(sequence)):\n",
    "                input_sequences.append(sequence[:i+1])\n",
    "        \n",
    "        # Pad sequences\n",
    "        self.max_sequence_length = max_length\n",
    "        input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='pre')\n",
    "        \n",
    "        # Create X and y\n",
    "        X = input_sequences[:, :-1]\n",
    "        y = input_sequences[:, -1]\n",
    "        \n",
    "        # Convert y to categorical\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        y = to_categorical(y, num_classes=self.vocab_size)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def save_tokenizer(self, filepath):\n",
    "        \"\"\"Menyimpan tokenizer\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "    \n",
    "    def load_tokenizer(self, filepath):\n",
    "        \"\"\"Memuat tokenizer\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd2ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model LSTM\n",
    "class LSTMLyricGenerator:\n",
    "    def __init__(self, vocab_size, max_sequence_length, embedding_dim=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Membangun model LSTM\"\"\"\n",
    "        self.model = Sequential([\n",
    "            Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_sequence_length-1),\n",
    "            LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(self.vocab_size, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train_model(self, X, y, epochs=50, batch_size=32, validation_split=0.2):\n",
    "        \"\"\"Melatih model\"\"\"\n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "        \n",
    "        # Training\n",
    "        history = self.model.fit(\n",
    "            X, y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop, checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def generate_text(self, tokenizer, seed_text, num_words=20, temperature=1.0):\n",
    "        \"\"\"Generate text menggunakan model yang sudah dilatih\"\"\"\n",
    "        result = seed_text\n",
    "        \n",
    "        for _ in range(num_words):\n",
    "            # Tokenize current text\n",
    "            token_list = tokenizer.texts_to_sequences([result])[0]\n",
    "            token_list = pad_sequences([token_list], maxlen=self.max_sequence_length-1, padding='pre')\n",
    "            \n",
    "            # Predict next word\n",
    "            predicted_probs = self.model.predict(token_list, verbose=0)[0]\n",
    "            \n",
    "            # Apply temperature for creativity\n",
    "            predicted_probs = np.log(predicted_probs + 1e-8) / temperature\n",
    "            predicted_probs = np.exp(predicted_probs)\n",
    "            predicted_probs = predicted_probs / np.sum(predicted_probs)\n",
    "            \n",
    "            # Sample from distribution\n",
    "            predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
    "            \n",
    "            # Convert back to word\n",
    "            output_word = \"\"\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == predicted_index:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            \n",
    "            if output_word:\n",
    "                result += \" \" + output_word\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24281137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fungsi Utama untuk Training\n",
    "def main_training():\n",
    "    print(\"=== LSTM Lyric Generator Training ===\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading dataset...\")\n",
    "    texts = download_poetry_dataset()\n",
    "    \n",
    "    # Preprocessing\n",
    "    print(\"Preprocessing data...\")\n",
    "    preprocessor = TextPreprocessor()\n",
    "    X, y = preprocessor.prepare_sequences(texts, vocab_size=5000, max_length=30)\n",
    "    \n",
    "    print(f\"Vocabulary size: {preprocessor.vocab_size}\")\n",
    "    print(f\"Sequence length: {preprocessor.max_sequence_length}\")\n",
    "    print(f\"Training samples: {len(X)}\")\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building model...\")\n",
    "    generator = LSTMLyricGenerator(\n",
    "        vocab_size=preprocessor.vocab_size,\n",
    "        max_sequence_length=preprocessor.max_sequence_length,\n",
    "        embedding_dim=100\n",
    "    )\n",
    "    \n",
    "    model = generator.build_model()\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    history = generator.train_model(X, y, epochs=100, batch_size=16)\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model.save('lstm_lyric_generator_model.h5')\n",
    "    preprocessor.save_tokenizer('tokenizer.pkl')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return generator, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Fungsi untuk Generate Lirik\n",
    "def generate_lyrics(seed_text=\"the sun\", num_words=30, temperature=0.8):\n",
    "    \"\"\"Generate lirik/puisi baru\"\"\"\n",
    "    try:\n",
    "        # Load model dan tokenizer\n",
    "        model = tf.keras.models.load_model('lstm_lyric_generator_model.h5')\n",
    "        \n",
    "        preprocessor = TextPreprocessor()\n",
    "        preprocessor.load_tokenizer('tokenizer.pkl')\n",
    "        \n",
    "        # Create generator instance\n",
    "        generator = LSTMLyricGenerator(\n",
    "            vocab_size=len(preprocessor.tokenizer.word_index) + 1,\n",
    "            max_sequence_length=30\n",
    "        )\n",
    "        generator.model = model\n",
    "        \n",
    "        # Generate text\n",
    "        generated_text = generator.generate_text(\n",
    "            preprocessor.tokenizer, \n",
    "            seed_text, \n",
    "            num_words=num_words, \n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Pastikan model sudah dilatih terlebih dahulu dengan menjalankan main_training()\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Fungsi untuk Evaluasi Model\n",
    "def evaluate_model():\n",
    "    \"\"\"Evaluasi model dengan berbagai seed text\"\"\"\n",
    "    seed_texts = [\n",
    "        \"love is\",\n",
    "        \"the moon\",\n",
    "        \"in the night\",\n",
    "        \"flowers bloom\",\n",
    "        \"time flows\"\n",
    "    ]\n",
    "    \n",
    "    temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "    \n",
    "    print(\"=== Generated Lyrics/Poetry ===\")\n",
    "    \n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\nSeed: '{seed}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_lyrics(seed, num_words=25, temperature=temp)\n",
    "            if generated:\n",
    "                print(f\"Temperature {temp}: {generated}\")\n",
    "        print()\n",
    "\n",
    "# 8. Fungsi Helper untuk Analisis\n",
    "def analyze_dataset(texts):\n",
    "    \"\"\"Analisis dataset\"\"\"\n",
    "    print(\"=== Dataset Analysis ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_texts = len(texts)\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    avg_words = total_words / total_texts\n",
    "    \n",
    "    print(f\"Total texts: {total_texts}\")\n",
    "    print(f\"Total words: {total_words}\")\n",
    "    print(f\"Average words per text: {avg_words:.2f}\")\n",
    "    \n",
    "    # Word frequency\n",
    "    all_words = \" \".join(texts).lower().split()\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    print(f\"Unique words: {len(word_freq)}\")\n",
    "    print(f\"Most common words: {word_freq.most_common(10)}\")\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "# 7. Fungsi untuk Evaluasi Model\n",
    "def evaluate_model():\n",
    "    \"\"\"Evaluasi model dengan berbagai seed text\"\"\"\n",
    "    seed_texts = [\n",
    "        \"love is\",\n",
    "        \"the moon\",\n",
    "        \"in the night\",\n",
    "        \"flowers bloom\",\n",
    "        \"time flows\"\n",
    "    ]\n",
    "    \n",
    "    temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "    \n",
    "    print(\"=== Generated Lyrics/Poetry ===\")\n",
    "    \n",
    "    for seed in seed_texts:\n",
    "        print(f\"\\nSeed: '{seed}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            generated = generate_lyrics(seed, num_words=25, temperature=temp)\n",
    "            if generated:\n",
    "                print(f\"Temperature {temp}: {generated}\")\n",
    "        print()\n",
    "\n",
    "# 8. Fungsi Helper untuk Analisis\n",
    "def analyze_dataset(texts):\n",
    "    \"\"\"Analisis dataset\"\"\"\n",
    "    print(\"=== Dataset Analysis ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_texts = len(texts)\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    avg_words = total_words / total_texts\n",
    "    \n",
    "    print(f\"Total texts: {total_texts}\")\n",
    "    print(f\"Total words: {total_words}\")\n",
    "    print(f\"Average words per text: {avg_words:.2f}\")\n",
    "    \n",
    "    # Word frequency\n",
    "    all_words = \" \".join(texts).lower().split()\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    print(f\"Unique words: {len(word_freq)}\")\n",
    "    print(f\"Most common words: {word_freq.most_common(10)}\")\n",
    "    \n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6028d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Contoh Penggunaan\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"LSTM Lyric/Poetry Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Pilihan menu\n",
    "    print(\"1. Train new model\")\n",
    "    print(\"2. Generate lyrics (requires trained model)\")\n",
    "    print(\"3. Evaluate model\")\n",
    "    print(\"4. Analyze dataset\")\n",
    "    \n",
    "    choice = input(\"Choose option (1-4): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        generator, preprocessor = main_training()\n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "    elif choice == \"2\":\n",
    "        seed = input(\"Enter seed text (or press Enter for default): \") or \"the sun\"\n",
    "        num_words = int(input(\"Number of words to generate (default 30): \") or 30)\n",
    "        temperature = float(input(\"Temperature (0.5-2.0, default 0.8): \") or 0.8)\n",
    "        \n",
    "        result = generate_lyrics(seed, num_words, temperature)\n",
    "        if result:\n",
    "            print(f\"\\nGenerated text: {result}\")\n",
    "        \n",
    "    elif choice == \"3\":\n",
    "        evaluate_model()\n",
    "        \n",
    "    elif choice == \"4\":\n",
    "        texts = download_poetry_dataset()\n",
    "        analyze_dataset(texts)\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid choice!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
